{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA\n",
      "=============\n",
      "F value: 4.999999999999998\n",
      "P value: 0.021683749320078414 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "data = np.rec.array([\n",
    "('Pat', 5),\n",
    "('Pat', 4),\n",
    "('Pat', 4),\n",
    "('Pat', 3),\n",
    "('Pat', 9),\n",
    "('Pat', 4),\n",
    "('Jack', 4),\n",
    "('Jack', 8),\n",
    "('Jack', 7),\n",
    "('Jack', 5),\n",
    "('Jack', 1),\n",
    "('Jack', 5),\n",
    "('Alex', 9),\n",
    "('Alex', 8),\n",
    "('Alex', 8),\n",
    "('Alex', 10),\n",
    "('Alex', 5),\n",
    "('Alex', 10)], dtype = [('Archer','|U5'),('Score', '<i8')])\n",
    " \n",
    "f, p = stats.f_oneway(data[data['Archer'] == 'Pat'].Score,\n",
    "                      data[data['Archer'] == 'Jack'].Score,\n",
    "                      data[data['Archer'] == 'Alex'].Score)\n",
    " \n",
    "print ('One-way ANOVA')\n",
    "print ('=============')\n",
    " \n",
    "print ('F value:', f)\n",
    "print ('P value:', p, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 4, 3, 9, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Archer'] == 'Pat'].Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  4,  4,  3,  9,  4,  4,  8,  7,  5,  1,  5,  9,  8,  8, 10,  5,\n",
       "       10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no field of name Ancher",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-641a134ef118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ancher'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/records.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, indx)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# copy behavior of getattr, except that here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: no field of name Ancher"
     ]
    }
   ],
   "source": [
    "data['Ancher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.sandbox.stats.multicomp.MultiComparison at 0x110df6b38>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc = MultiComparison(data['Score'], data['Archer'])\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mc.tukeyhsd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=============================================\n",
      "group1 group2 meandiff  lower   upper  reject\n",
      "---------------------------------------------\n",
      " Alex   Jack  -3.3333  -6.5755 -0.0911  True \n",
      " Alex   Pat     -3.5   -6.7422 -0.2578  True \n",
      " Jack   Pat   -0.1667  -3.4089  3.0755 False \n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alex' 'Jack' 'Pat']\n"
     ]
    }
   ],
   "source": [
    "print(mc.groupsunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  4,  4,  3,  9,  4,  4,  8,  7,  5,  1,  5,  9,  8,  8, 10,  5,\n",
       "       10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pat', 'Pat', 'Pat', 'Pat', 'Pat', 'Pat', 'Jack', 'Jack', 'Jack',\n",
       "       'Jack', 'Jack', 'Jack', 'Alex', 'Alex', 'Alex', 'Alex', 'Alex',\n",
       "       'Alex'], dtype='<U5')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Archer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MultiComparison in module statsmodels.sandbox.stats.multicomp:\n",
      "\n",
      "class MultiComparison(builtins.object)\n",
      " |  MultiComparison(data, groups, group_order=None)\n",
      " |  \n",
      " |  Tests for multiple comparisons\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  data : array\n",
      " |      independent data samples\n",
      " |  groups : array\n",
      " |      group labels corresponding to each data point\n",
      " |  group_order : list of strings, optional\n",
      " |      the desired order for the group mean results to be reported in. If\n",
      " |      not specified, results are reported in increasing order.\n",
      " |      If group_order does not contain all labels that are in groups, then\n",
      " |      only those observations are kept that have a label in group_order.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data, groups, group_order=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  allpairtest(self, testfunc, alpha=0.05, method='bonf', pvalidx=1)\n",
      " |      run a pairwise test on all pairs with multiple test correction\n",
      " |      \n",
      " |      The statistical test given in testfunc is calculated for all pairs\n",
      " |      and the p-values are adjusted by methods in multipletests. The p-value\n",
      " |      correction is generic and based only on the p-values, and does not\n",
      " |      take any special structure of the hypotheses into account.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      testfunc : function\n",
      " |          A test function for two (independent) samples. It is assumed that\n",
      " |          the return value on position pvalidx is the p-value.\n",
      " |      alpha : float\n",
      " |          familywise error rate\n",
      " |      method : string\n",
      " |          This specifies the method for the p-value correction. Any method\n",
      " |          of multipletests is possible.\n",
      " |      pvalidx : int (default: 1)\n",
      " |          position of the p-value in the return of testfunc\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sumtab : SimpleTable instance\n",
      " |          summary table for printing\n",
      " |      \n",
      " |      errors:  TODO: check if this is still wrong, I think it's fixed.\n",
      " |      results from multipletests are in different order\n",
      " |      pval_corrected can be larger than 1 ???\n",
      " |  \n",
      " |  getranks(self)\n",
      " |      convert data to rankdata and attach\n",
      " |      \n",
      " |      \n",
      " |      This creates rankdata as it is used for non-parametric tests, where\n",
      " |      in the case of ties the average rank is assigned.\n",
      " |  \n",
      " |  kruskal(self, pairs=None, multimethod='T')\n",
      " |      pairwise comparison for kruskal-wallis test\n",
      " |      \n",
      " |      This is just a reimplementation of scipy.stats.kruskal and does\n",
      " |      not yet use a multiple comparison correction.\n",
      " |  \n",
      " |  tukeyhsd(self, alpha=0.05)\n",
      " |      Tukey's range test to compare means of all pairs of groups\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alpha : float, optional\n",
      " |          Value of FWER at which to calculate HSD.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results : TukeyHSDResults instance\n",
      " |          A results class containing relevant data and some post-hoc\n",
      " |          calculations\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MultiComparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
